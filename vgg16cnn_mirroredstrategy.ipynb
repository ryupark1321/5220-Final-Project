{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f72e7c-de07-48c5-b906-2cf4a73d5feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import MaxPooling2D, Flatten, Dense, Conv2D, Rescaling\n",
    "import os\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85818565-3c93-4135-bda6-b9475c25c2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a052073b-07ae-4572-9456-3d6e9bf318d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9469 files belonging to 10 classes.\n",
      "Found 3925 files belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 22:49:38.172469: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 9469\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\026TensorSliceDataset:585\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2024-04-30 22:49:38.181155: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 3925\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\026TensorSliceDataset:592\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data and distribute it\n",
    "\n",
    "batch_size = 128\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "scratch = os.environ['SCRATCH']\n",
    "train_dir = os.path.join(scratch,'imagenette/imagenette2/train/')\n",
    "val_dir = os.path.join(scratch,'imagenette/imagenette2/val/')\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# with strategy.scope():\n",
    "  # Create the layer(s) under scope.\n",
    "\n",
    "def dataset_fn(input_context):\n",
    "  # a tf.data.Dataset\n",
    "  dataset = train_dataset\n",
    "\n",
    "  # Custom your batching, sharding, prefetching, etc.\n",
    "  global_batch_size = 128\n",
    "  batch_siz = input_context.get_per_replica_batch_size(global_batch_size)\n",
    "  dataset = dataset.batch(batch_siz, drop_remainder=True)\n",
    "  dataset = dataset.shard(\n",
    "      input_context.num_input_pipelines,\n",
    "      input_context.input_pipeline_id\n",
    "  )\n",
    "  return dataset\n",
    "# with strategy.scope():\n",
    "    \n",
    "# distributed_dataset = strategy.distribute_datasets_from_function(dataset_fn)\n",
    "distributed_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dist_dataset = strategy.experimental_distribute_dataset(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1df80ecd-415c-47c6-abc3-66486308ff1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "def vgg16():\n",
    "    model = Sequential([\n",
    "        Rescaling(1./255),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3), strides=1),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dense(1000, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b3c98ce9-72f1-4827-803b-70a4d7504560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4c99e43-675f-44b9-83d3-05d2aaa83263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  # Set reduction to `NONE` so you can do the reduction yourself.\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True,\n",
    "      reduction=tf.keras.losses.Reduction.NONE)\n",
    "  def compute_loss(labels, predictions, model_losses):\n",
    "    per_example_loss = loss_object(labels, predictions)\n",
    "    loss = tf.nn.compute_average_loss(per_example_loss)\n",
    "    if model_losses:\n",
    "      loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "058039a3-3361-4852-8e4b-26cc18dd6d96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='train_accuracy')\n",
    "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b2b1d123-5f84-479f-8ad9-ba9ee3488808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A model, an optimizer, and a checkpoint must be created under `strategy.scope`.\n",
    "with strategy.scope():\n",
    "  model = vgg16()\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "51f62ca2-6438-4b8c-b4a2-900514208263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(inputs):\n",
    "  images, labels = inputs\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(images, training=True)\n",
    "    loss = compute_loss(labels, predictions, model.losses)\n",
    "\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_accuracy.update_state(labels, predictions)\n",
    "  return loss\n",
    "\n",
    "def test_step(inputs):\n",
    "  images, labels = inputs\n",
    "\n",
    "  predictions = model(images, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss.update_state(t_loss)\n",
    "  test_accuracy.update_state(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "920d74ff-7911-41c1-a563-a5bf7f1b087d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12c25f-2c82-45e8-ab17-bfce1c0ffe5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1\n",
      "Epoch 1, Loss: 2.8015942573547363, Accuracy: 11.0043306350708, Test Loss: 2.3056492805480957, Test Accuracy: 11.745223045349121\n",
      "Epoch 2, Loss: 2.2550034523010254, Accuracy: 17.752666473388672, Test Loss: 2.2164299488067627, Test Accuracy: 19.439489364624023\n",
      "Epoch 3, Loss: 2.0420689582824707, Accuracy: 27.83820915222168, Test Loss: 1.9922412633895874, Test Accuracy: 31.03184700012207\n",
      "Epoch 4, Loss: 1.7149308919906616, Accuracy: 41.472171783447266, Test Loss: 1.664320707321167, Test Accuracy: 44.356689453125\n",
      "Epoch 5, Loss: 1.4539991617202759, Accuracy: 51.65275955200195, Test Loss: 1.4173789024353027, Test Accuracy: 52.178340911865234\n",
      "Epoch 6, Loss: 1.2592525482177734, Accuracy: 58.49614715576172, Test Loss: 1.4011811017990112, Test Accuracy: 53.47770309448242\n",
      "Epoch 7, Loss: 1.116281270980835, Accuracy: 62.731014251708984, Test Loss: 1.2571011781692505, Test Accuracy: 59.94904708862305\n",
      "Epoch 8, Loss: 0.9850564002990723, Accuracy: 67.27214813232422, Test Loss: 1.1782466173171997, Test Accuracy: 62.802547454833984\n"
     ]
    }
   ],
   "source": [
    "# `run` replicates the provided computation and runs it\n",
    "# with the distributed input.\n",
    "@tf.function\n",
    "def distributed_train_step(dataset_inputs):\n",
    "  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
    "  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                         axis=None)\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step(dataset_inputs):\n",
    "  return strategy.run(test_step, args=(dataset_inputs,))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # TRAIN LOOP\n",
    "  total_loss = 0.0\n",
    "  num_batches = 0\n",
    "  for x in distributed_dataset:\n",
    "    total_loss += distributed_train_step(x)\n",
    "    num_batches += 1\n",
    "  train_loss = total_loss / num_batches\n",
    "\n",
    "  # TEST LOOP\n",
    "  for x in test_dist_dataset:\n",
    "    distributed_test_step(x)\n",
    "\n",
    "  if epoch % 2 == 0:\n",
    "    checkpoint.save(checkpoint_prefix)\n",
    "\n",
    "  template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
    "              \"Test Accuracy: {}\")\n",
    "  print(template.format(epoch + 1, train_loss,\n",
    "                         train_accuracy.result() * 100, test_loss.result(),\n",
    "                         test_accuracy.result() * 100))\n",
    "\n",
    "  test_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402518e-b879-421b-9854-bffb7348db7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15504a-5303-4cd0-97d4-4d516147a9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.9.0",
   "language": "python",
   "name": "tensorflow-2.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
