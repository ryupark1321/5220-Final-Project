{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f72e7c-de07-48c5-b906-2cf4a73d5feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import MaxPooling2D, Flatten, Dense, Conv2D, Rescaling\n",
    "import os\n",
    "import time\n",
    "# import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85818565-3c93-4135-bda6-b9475c25c2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 13:58:46.472734: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-14 13:58:49.117957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38218 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:03:00.0, compute capability: 8.0\n",
      "2024-05-14 13:58:49.127138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38218 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n",
      "2024-05-14 13:58:49.128750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38218 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:82:00.0, compute capability: 8.0\n",
      "2024-05-14 13:58:49.130329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38218 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a052073b-07ae-4572-9456-3d6e9bf318d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9469 files belonging to 10 classes.\n",
      "Found 3925 files belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 13:58:50.695330: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 9469\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2024-05-14 13:58:50.702760: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 3925\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:7\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data and distribute it\n",
    "\n",
    "batch_size = 128\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "scratch = os.environ['SCRATCH']\n",
    "train_dir = os.path.join(scratch,'imagenette/imagenette2/train/')\n",
    "val_dir = os.path.join(scratch,'imagenette/imagenette2/val/')\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# with strategy.scope():\n",
    "  # Create the layer(s) under scope.\n",
    "\n",
    "def dataset_fn(input_context):\n",
    "  # a tf.data.Dataset\n",
    "  dataset = train_dataset\n",
    "\n",
    "  # Custom your batching, sharding, prefetching, etc.\n",
    "  global_batch_size = 128\n",
    "  batch_siz = input_context.get_per_replica_batch_size(global_batch_size)\n",
    "  dataset = dataset.batch(batch_siz, drop_remainder=True)\n",
    "  dataset = dataset.shard(\n",
    "      input_context.num_input_pipelines,\n",
    "      input_context.input_pipeline_id\n",
    "  )\n",
    "  return dataset\n",
    "# with strategy.scope():\n",
    "    \n",
    "# distributed_dataset = strategy.distribute_datasets_from_function(dataset_fn)\n",
    "distributed_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dist_dataset = strategy.experimental_distribute_dataset(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977d5fac-3930-4393-9779-9f08dd82a5a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.take(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df80ecd-415c-47c6-abc3-66486308ff1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "def vgg16():\n",
    "    model = Sequential([\n",
    "        Rescaling(1./255),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3), strides=1),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same', strides=1),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dense(1000, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3c98ce9-72f1-4827-803b-70a4d7504560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c99e43-675f-44b9-83d3-05d2aaa83263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  # Set reduction to `NONE` so you can do the reduction yourself.\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True,\n",
    "      reduction=tf.keras.losses.Reduction.NONE)\n",
    "  def compute_loss(labels, predictions, model_losses):\n",
    "    per_example_loss = loss_object(labels, predictions)\n",
    "    loss = tf.nn.compute_average_loss(per_example_loss)\n",
    "    if model_losses:\n",
    "      loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "058039a3-3361-4852-8e4b-26cc18dd6d96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='train_accuracy')\n",
    "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b1d123-5f84-479f-8ad9-ba9ee3488808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A model, an optimizer, and a checkpoint must be created under `strategy.scope`.\n",
    "with strategy.scope():\n",
    "  model = vgg16()\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51f62ca2-6438-4b8c-b4a2-900514208263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(inputs):\n",
    "  images, labels = inputs\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(images, training=True)\n",
    "    loss = compute_loss(labels, predictions, model.losses)\n",
    "\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_accuracy.update_state(labels, predictions)\n",
    "  return loss\n",
    "\n",
    "def test_step(inputs):\n",
    "  images, labels = inputs\n",
    "\n",
    "  predictions = model(images, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss.update_state(t_loss)\n",
    "  test_accuracy.update_state(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920d74ff-7911-41c1-a563-a5bf7f1b087d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b12c25f-2c82-45e8-ab17-bfce1c0ffe5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.9.0/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 13:59:28.220087: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8302\n",
      "2024-05-14 13:59:28.319274: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8302\n",
      "2024-05-14 13:59:28.432360: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8302\n",
      "2024-05-14 13:59:28.601004: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8302\n",
      "2024-05-14 13:59:32.177188: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 32 all-reduces with algorithm = nccl, num_packs = 1\n",
      "Time: 36.23625349998474 Epoch 1, Loss: 2.696596622467041, Accuracy: 12.345548629760742, Test Loss: 2.2625107765197754, Test Accuracy: 15.97452163696289\n",
      "Time: 43.44406843185425 Epoch 2, Loss: 2.1876394748687744, Accuracy: 20.889217376708984, Test Loss: 2.105328321456909, Test Accuracy: 23.69426727294922\n",
      "Time: 56.63681125640869 Epoch 3, Loss: 1.8827011585235596, Accuracy: 35.230751037597656, Test Loss: 1.868722915649414, Test Accuracy: 37.55413818359375\n",
      "Time: 63.7699818611145 Epoch 4, Loss: 1.592390537261963, Accuracy: 46.60470962524414, Test Loss: 1.4918851852416992, Test Accuracy: 50.31847381591797\n",
      "Time: 77.03864622116089 Epoch 5, Loss: 1.3699631690979004, Accuracy: 54.905479431152344, Test Loss: 1.3947936296463013, Test Accuracy: 54.929935455322266\n",
      "Time: 84.04182529449463 Epoch 6, Loss: 1.1886829137802124, Accuracy: 61.00960922241211, Test Loss: 1.2715445756912231, Test Accuracy: 60.76433181762695\n",
      "Time: 96.81319570541382 Epoch 7, Loss: 0.9454789161682129, Accuracy: 68.75066375732422, Test Loss: 1.3526188135147095, Test Accuracy: 59.56687927246094\n",
      "Time: 103.82893872261047 Epoch 8, Loss: 0.7545333504676819, Accuracy: 75.09768676757812, Test Loss: 1.5411335229873657, Test Accuracy: 58.87898254394531\n",
      "Time: 116.03209328651428 Epoch 9, Loss: 0.49085715413093567, Accuracy: 83.63079071044922, Test Loss: 1.7347944974899292, Test Accuracy: 60.12738800048828\n",
      "Time: 123.05546998977661 Epoch 10, Loss: 0.2654437720775604, Accuracy: 91.35072326660156, Test Loss: 2.19299578666687, Test Accuracy: 59.59235382080078\n",
      "Time: 136.60554027557373 Epoch 11, Loss: 0.15093985199928284, Accuracy: 95.17372131347656, Test Loss: 2.669861078262329, Test Accuracy: 58.16560363769531\n",
      "Time: 143.61741375923157 Epoch 12, Loss: 0.09118843078613281, Accuracy: 97.2013931274414, Test Loss: 2.618643283843994, Test Accuracy: 58.292991638183594\n",
      "Time: 158.39315223693848 Epoch 13, Loss: 0.07029755413532257, Accuracy: 97.66606903076172, Test Loss: 2.9487264156341553, Test Accuracy: 55.8216552734375\n",
      "Time: 165.45082902908325 Epoch 14, Loss: 0.06539158523082733, Accuracy: 98.05681610107422, Test Loss: 2.6568362712860107, Test Accuracy: 58.14012908935547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 14:02:12.659263: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:138 : RESOURCE_EXHAUSTED: training_checkpoints/ckpt-8_temp/part-00000-of-00001.data-00000-of-00001.tempstate11898694674970125887; Disk quota exceeded\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "training_checkpoints/ckpt-8_temp/part-00000-of-00001.data-00000-of-00001.tempstate11898694674970125887; Disk quota exceeded [Op:SaveV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m   distributed_test_step(x)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 29\u001b[0m   \u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m time_start\n\u001b[1;32m     31\u001b[0m template \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.9.0/lib/python3.9/site-packages/tensorflow/python/training/tracking/util.py:2352\u001b[0m, in \u001b[0;36mCheckpoint.save\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m   2349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2350\u001b[0m   checkpoint_number \u001b[38;5;241m=\u001b[39m assign_op\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m-> 2352\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_number\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_ckpt_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;66;03m# Update internal checkpoint state.\u001b[39;00m\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39mexperimental_enable_async_checkpoint:\n\u001b[1;32m   2359\u001b[0m   \u001b[38;5;66;03m# For synchronous checkpoint, since SaveOp may be run as in graph/session\u001b[39;00m\n\u001b[1;32m   2360\u001b[0m   \u001b[38;5;66;03m# mode, checkpoint state can only be updated once SaveOp is finished by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[38;5;66;03m# as the eager SaveOp finishes.\u001b[39;00m\n\u001b[1;32m   2366\u001b[0m   \u001b[38;5;66;03m# See TrackableSaver._save_cached_when_graph_building() for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.9.0/lib/python3.9/site-packages/tensorflow/python/training/tracking/util.py:2236\u001b[0m, in \u001b[0;36mCheckpoint._write\u001b[0;34m(self, file_prefix, options, update_ckpt_state)\u001b[0m\n\u001b[1;32m   2234\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2235\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;129;01mor\u001b[39;00m checkpoint_options\u001b[38;5;241m.\u001b[39mCheckpointOptions()\n\u001b[0;32m-> 2236\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2238\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_ckpt_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_ckpt_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2240\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2242\u001b[0m metrics\u001b[38;5;241m.\u001b[39mAddCheckpointWriteDuration(\n\u001b[1;32m   2243\u001b[0m     api_label\u001b[38;5;241m=\u001b[39m_CHECKPOINT_V2,\n\u001b[1;32m   2244\u001b[0m     microseconds\u001b[38;5;241m=\u001b[39m_get_duration_microseconds(start_time, end_time))\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.9.0/lib/python3.9/site-packages/tensorflow/python/training/tracking/util.py:1326\u001b[0m, in \u001b[0;36mTrackableSaver.save\u001b[0;34m(self, file_prefix, checkpoint_number, session, options, update_ckpt_state)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor_util\u001b[38;5;241m.\u001b[39mis_tensor(file_prefix):\n\u001b[1;32m   1324\u001b[0m   file_io\u001b[38;5;241m.\u001b[39mrecursive_create_dir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(file_prefix))\n\u001b[0;32m-> 1326\u001b[0m save_path, new_feed_additions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_cached_when_graph_building\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_prefix_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_graph_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_ckpt_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_feed_additions:\n\u001b[1;32m   1330\u001b[0m   feed_dict\u001b[38;5;241m.\u001b[39mupdate(new_feed_additions)\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.9.0/lib/python3.9/site-packages/tensorflow/python/training/tracking/util.py:1271\u001b[0m, in \u001b[0;36mTrackableSaver._save_cached_when_graph_building\u001b[0;34m(self, file_prefix, object_graph_tensor, options, update_ckpt_state)\u001b[0m\n\u001b[1;32m   1268\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_save_operation, feed_additions\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;66;03m# Execute the normal checkpoint, i.e., synchronous.\u001b[39;00m\n\u001b[0;32m-> 1271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.9.0/lib/python3.9/site-packages/tensorflow/python/training/tracking/util.py:1217\u001b[0m, in \u001b[0;36mTrackableSaver._save_cached_when_graph_building.<locals>._run_save\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_save_object_graph \u001b[38;5;241m!=\u001b[39m graph_proto\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;66;03m# When executing eagerly, we need to re-create SaveableObjects each\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# time save() is called so they pick up new Tensors passed to their\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# constructors. That means the Saver needs to be copied with a new\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# var_list.\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function()):\n\u001b[1;32m   1215\u001b[0m   saver \u001b[38;5;241m=\u001b[39m functional_saver\u001b[38;5;241m.\u001b[39mMultiDeviceSaver(named_saveable_objects,\n\u001b[1;32m   1216\u001b[0m                                             registered_savers)\n\u001b[0;32m-> 1217\u001b[0m   save_op \u001b[38;5;241m=\u001b[39m \u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1218\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/cpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([save_op]):\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.9.0/lib/python3.9/site-packages/tensorflow/python/training/saving/functional_saver.py:375\u001b[0m, in \u001b[0;36mMultiDeviceSaver.save\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    373\u001b[0m   tf_function_save()\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msave_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.9.0/lib/python3.9/site-packages/tensorflow/python/training/saving/functional_saver.py:349\u001b[0m, in \u001b[0;36mMultiDeviceSaver.save.<locals>.save_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    344\u001b[0m   saved_prefixes\u001b[38;5;241m.\u001b[39mappend(shard_prefix)\n\u001b[1;32m    345\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;66;03m# _SingleDeviceSaver will use the CPU device when necessary, but\u001b[39;00m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;66;03m# initial read operations should be placed on the SaveableObject's\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# device.\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     sharded_saves\u001b[38;5;241m.\u001b[39mappend(\u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies(sharded_saves):\n\u001b[1;32m    352\u001b[0m   \u001b[38;5;66;03m# Merge on the io_device if specified, otherwise co-locates the merge op\u001b[39;00m\n\u001b[1;32m    353\u001b[0m   \u001b[38;5;66;03m# with the last device used.\u001b[39;00m\n\u001b[1;32m    354\u001b[0m   merge_device \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    355\u001b[0m       options\u001b[38;5;241m.\u001b[39mexperimental_io_device \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    356\u001b[0m       saveable_object_util\u001b[38;5;241m.\u001b[39mset_cpu0(last_device))\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.9.0/lib/python3.9/site-packages/tensorflow/python/training/saving/functional_saver.py:80\u001b[0m, in \u001b[0;36m_SingleDeviceSaver.save\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m     78\u001b[0m save_device \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mexperimental_io_device \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(save_device):\n\u001b[0;32m---> 80\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_slices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.9.0/lib/python3.9/site-packages/tensorflow/python/ops/gen_io_ops.py:1707\u001b[0m, in \u001b[0;36msave_v2\u001b[0;34m(prefix, tensor_names, shape_and_slices, tensors, name)\u001b[0m\n\u001b[1;32m   1705\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1706\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1707\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msave_v2_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m      \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_and_slices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m   1711\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.9.0/lib/python3.9/site-packages/tensorflow/python/ops/gen_io_ops.py:1728\u001b[0m, in \u001b[0;36msave_v2_eager_fallback\u001b[0;34m(prefix, tensor_names, shape_and_slices, tensors, name, ctx)\u001b[0m\n\u001b[1;32m   1726\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [prefix, tensor_names, shape_and_slices] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(tensors)\n\u001b[1;32m   1727\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_dtypes)\n\u001b[0;32m-> 1728\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSaveV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _result\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.9.0/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: training_checkpoints/ckpt-8_temp/part-00000-of-00001.data-00000-of-00001.tempstate11898694674970125887; Disk quota exceeded [Op:SaveV2]"
     ]
    }
   ],
   "source": [
    "# `run` replicates the provided computation and runs it\n",
    "# with the distributed input.\n",
    "@tf.function\n",
    "def distributed_train_step(dataset_inputs):\n",
    "  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
    "  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                         axis=None)\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step(dataset_inputs):\n",
    "  return strategy.run(test_step, args=(dataset_inputs,))\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # TRAIN LOOP\n",
    "  total_loss = 0.0\n",
    "  num_batches = 0\n",
    "  for x in distributed_dataset:\n",
    "    total_loss += distributed_train_step(x)\n",
    "    num_batches += 1\n",
    "  train_loss = total_loss / num_batches\n",
    "\n",
    "  # TEST LOOP\n",
    "  for x in test_dist_dataset:\n",
    "    distributed_test_step(x)\n",
    "\n",
    "  if epoch % 2 == 0:\n",
    "    # checkpoint.save(checkpoint_prefix)\n",
    "  total_time = time.time() - time_start\n",
    "  template = (\"Time: {} Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
    "              \"Test Accuracy: {}\")\n",
    "  print(template.format(total_time, epoch + 1, train_loss,\n",
    "                         train_accuracy.result() * 100, test_loss.result(),\n",
    "                         test_accuracy.result() * 100))\n",
    "\n",
    "  test_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402518e-b879-421b-9854-bffb7348db7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15504a-5303-4cd0-97d4-4d516147a9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.9.0",
   "language": "python",
   "name": "tensorflow-2.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
